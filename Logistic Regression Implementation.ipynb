{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image data needs to be flattened before it can be fed to the model.\n",
    "#### e.g. an array of m 64x64x3 images should be converted into a (64x64x3) by m matrix.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_orig, x_test_orig = np.random.randint(255, size = (10,64,64,3)),np.random.randint(255, size = (2,64,64,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = np.random.randint(2, size = (1,10)), np.random.randint(2, size = (1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(input):\n",
    "    return (input.reshape(input.shape[0], -1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_flat, x_test_flat = flatten(x_train_orig), flatten(x_test_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 10)\n",
      "(12288, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_flat.shape)\n",
    "print(x_test_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 0 0 0 1 1]]\n",
      "[[0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the pixel data by dividing the array by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_flat/255\n",
    "x_test = x_test_flat/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid function\n",
    "\n",
    "sigmoid = 1/(1+exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter initializer\n",
    "Logistic regression has two parameters: <b>w</b>(weights) which is a vector of dimension <b>x_train.shape[0]</b> i.e. one for each input feature and <b>b</b>(bias) which is a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(dim):\n",
    "    b = 0\n",
    "    w = np.zeros(dim,1)\n",
    "    \n",
    "    \n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return b, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward and backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>In the forward pass we calculate the final predictions and the total cost.</li>\n",
    "<br>\n",
    "<li>For the backwards pass we calculate the gradient of the cost with respect to our parameters w and b.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    #Number of training examples to calculate mean\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    #Forward propagation (from prediction to cost)\n",
    "    \n",
    "    #Make a prediction\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "    #Calculate cost i.e loss summed over all training examples\n",
    "    cost = (-1/m)*np.sum(np.dot(Y, (np.log(A).T)) + np.dot((1-Y), (np.log(1-A).T)))\n",
    "    \n",
    "    \n",
    "    #Backward propagation(finding gradients)\n",
    "    \n",
    "    #Gradient of cost w.r.t 'w'\n",
    "    dw = (1/m)*np.dot(X,(A-Y).T)\n",
    "    \n",
    "    #Gradient of cost w.r.t 'b'\n",
    "    db = (1/m)*np.sum(A-Y)\n",
    "    \n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads,cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(w, b, X, Y, num_iter, learning_rate, print_cost = False):\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        \n",
    "        #calculate derivatives and cost\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        #retrieve cacluated derivatives\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        #Update paramenters \n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "        \n",
    "        #log the cost every 100 iterations\n",
    "        if(i % 100 == 0):\n",
    "            costs.append(cost)\n",
    "        \n",
    "        \n",
    "        if(print_cost and i % 100 == 0):\n",
    "            print(\"Cost after iteration %i: %f\"%(i,cost))\n",
    "            \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    \n",
    "    # Number of training examples is m\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Vector to store predictions\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    \n",
    "    # Ensuring w is the correct shape\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    A = sigmoid((np.dot(w.T, X) + b))\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        if(A[0, i] > 0.5):\n",
    "            Y_prediction[0, i] = 1\n",
    "        else:\n",
    "            Y_prediction[0, i] = 0\n",
    "    \n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all functions into a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(w, b, X_train, Y_train, X_test, Y_test, num_iter = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \n",
    "    # Initialize parameters\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "    \n",
    "    # Perform Gradient Descent\n",
    "    params, grads, costs = optimizer(w, b, X_train, Y_train, num_iter, learning_rate, print_cost = False)\n",
    "    \n",
    "    # Retrieve updated parameters from 'params' dict\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    \n",
    "    #Use w and b to make predictions on test set\n",
    "    predictions_test  = predict(w, b, X_test)\n",
    "    predictions_train = predict(w, b, X_train)\n",
    "    \n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(predictions_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(predictions_test - Y_test)) * 100))\n",
    "    \n",
    "    \n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"predictions_test\": predictions_test, \n",
    "         \"predictions_train\" : predictions_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iter\": num_iter}\n",
    "    \n",
    "    return d\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
